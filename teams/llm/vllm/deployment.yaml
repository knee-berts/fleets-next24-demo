# apiVersion: apps/v1
# kind: Deployment
# metadata:
#   name: vllm-gemma-deployment
#   namespace: vllm-inference
# spec:
#   replicas: 1
#   selector:
#     matchLabels:
#       app: gemma-server
#   template:
#     metadata:
#       labels:
#         app: gemma-server
#         ai.gke.io/model: gemma-3-4b-it
#         ai.gke.io/inference-server: vllm
#         examples.ai.gke.io/source: user-guide
#     spec:
#       containers:
#       - name: inference-server
#         image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250312_0916_RC01
#         resources:
#           requests:
#             cpu: "2"
#             memory: "20Gi"
#             ephemeral-storage: "20Gi"
#             nvidia.com/gpu: "1"
#           limits:
#             cpu: "2"
#             memory: "20Gi"
#             ephemeral-storage: "20Gi"
#             nvidia.com/gpu: "1"
#         command: ["python3", "-m", "vllm.entrypoints.openai.api_server"]
#         args:
#         - --model=$(MODEL_ID)
#         - --tensor-parallel-size=1
#         - --host=0.0.0.0
#         - --port=8000
#         - --max-model-len=32768
#         - --max-num-seqs=4
#         env:
#         - name: MODEL_ID
#           value: google/gemma-3-4b-it
#         - name: HUGGING_FACE_HUB_TOKEN
#           valueFrom:
#             secretKeyRef:
#               name: hf-secret
#               key: hf_api_token
#         volumeMounts:
#         - mountPath: /dev/shm
#           name: dshm
#       volumes:
#       - name: dshm
#         emptyDir:
#             medium: Memory
#       nodeSelector:
#         cloud.google.com/gke-accelerator: nvidia-l4
#         cloud.google.com/gke-gpu-driver-version: latest